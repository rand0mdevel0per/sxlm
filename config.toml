# SXLM Quila Configuration

[model]
dim = 1024
num_heads = 16
num_layers = 60
max_seq_len = 128000

[training]
learning_rate = 1e-4
el_trace_decay = 0.9
batch_size = 4
gradient_accumulation_steps = 8
max_steps = 100000
warmup_steps = 1000
save_steps = 5000
logging_steps = 100

# Auto-transition from SFT to semi-supervised
sft_steps = 30000
semi_supervised_steps = 70000

[hot]
hot_threshold = 0.5
global_heads = 4
local_heads = 4
selector_heads = 4
local_window = 512

[engram]
num_hash_tables = 8
max_ngram_len = 5

[sct]
branching_factor = 32
max_depth = 27

[storage]
gpu_cache_mb = 8192
ram_cache_mb = 32768
